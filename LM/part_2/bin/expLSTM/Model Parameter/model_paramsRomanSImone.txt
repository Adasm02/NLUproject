# This file is used to run your functions and print the results
# Please write your fuctions or classes in the functions.py

from functions import *
from utils import batch_collate_fn, load_datasets, store_model_with_params_and_lr

from functools import partial
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from tqdm import tqdm
import copy
import argparse
import numpy as np
import math

#Device selection
if torch.cuda.is_available():
    device = 'cuda'
    torch.cuda.empty_cache()
else:
    device = 'cpu'


# Configurable Parameters
params = {
    'emb_size': 800,
    'hid_size': 800,
    'lr': 1.5,
    'lr_decay': 0.75,
    'lr_decay_epoch' : 5,
    'NMT-ASGD' : True,
    'threshold' : 3,
    'clip': 5,
    'emb_dropout': 0.2,
    'out_dropout': 0.2,
    'n_epochs': 50,
    'patience': 5,
    'batch_size': 64,
    'weight_tying' : True,
    'drop_out_mode' : 'variational',
    'optimizer_type': 'SGD',
    'exp_name': 'expLSTM',
    'mode': 'train'
}
emb_size = params['emb_size']
hid_size = params['hid_size']
lr = params['lr']
lr_decay = params['lr_decay']
lr_decay_epoch = params['lr_decay_epoch']
asgd= params['NMT-ASGD']
threshold = params['threshold']
clip = params['clip']
emb_dropout = params['emb_dropout']
out_dropout = params['out_dropout']
n_epochs = params['n_epochs']
count_patience = params['patience']
batch_size = params['batch_size']
weight_tying = params['weight_tying']
drop_out_mode = params['drop_out_mode']
opt_type = params['optimizer_type']
exp_name = params['exp_name']
mode = params['mode']


if __name__ == "__main__":
    print("running part B"
    "")
    if mode == 'test':
        # Load test dataset
        _, _, test_dataset, lang = load_datasets()
        test_loader = DataLoader(
            test_dataset, batch_size=32, collate_fn=partial(batch_collate_fn, padding_token=lang.word_to_id["<pad>"], device=device) 
        )
        
        # Load model from checkpoint
        model, criterion_eval = load_checkpoint(
            emb_size, hid_size, lr, clip, device, emb_dropout, out_dropout, lang, opt_type,exp_name
        )
        
        # Evaluate model
        final_ppl, _ = evaluate_model(test_loader, criterion_eval, model)
        print('Test PPL: ', final_ppl)

    else:  # Training mode
        # Load datasets
        train_dataset, dev_dataset, test_dataset, lang = load_datasets()
        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, collate_fn=partial(batch_collate_fn, padding_token=lang.word_to_id["<pad>"],device=device), shuffle=True
        )
        dev_loader = DataLoader(
            dev_dataset, batch_size=32, collate_fn=partial(batch_collate_fn, padding_token=lang.word_to_id["<pad>"],device=device)
        )
        test_loader = DataLoader(
            test_dataset, batch_size=32, collate_fn=partial(batch_collate_fn, padding_token=lang.word_to_id["<pad>"],device=device)
        )

        # Initialize model, optimizer, and loss functions
        model, optimizer, criterion_train, criterion_eval = create_language_model(
            emb_size, hid_size, lr, lr_decay_epoch, lr_decay, clip, device, emb_dropout, out_dropout, lang, opt_type, weight_tying, drop_out_mode
        )

        # Training loop
        best_ppl = math.inf
        patience_counter = count_patience
        best_model = None
        losses_train, losses_dev, sampled_epochs, ppl_dev_list = [], [], [], []
        pbar = tqdm(range(1, n_epochs + 1))


        #for NT_ASGD
        best_dev_loss = []
        
        compute_avg = False

        for epoch in pbar:
            loss = train_model(train_loader, optimizer, criterion_train, model, clip)
            sampled_epochs.append(epoch)
            losses_train.append(np.asarray(loss).mean())
            #scheduler.step() #Reduce the lr of lr_decay percentage after every lr_decay_epoch

            if 't0' in optimizer.param_groups[0]:

                # Save the current weights and load the averaged ones
                tmp = {}
                for prm in model.parameters():
                    tmp[prm] = prm.data.clone()
                    prm.data = optimizer.state[prm]['ax'].clone()
                    
                # Evaluate the model with the averaged weights
                ppl_dev, loss_dev = evaluate_model(dev_loader, criterion_eval, model)
                ppl_dev_list.append(ppl_dev)
                losses_dev.append(np.asarray(loss_dev).mean())
                pbar.set_description(f"lr= {params['lr']} ASGD= {'t0' in optimizer.param_groups[0]} PPL: {ppl_dev}")       

                # Restore the current weights
                for prm in model.parameters():
                    prm.data = tmp[prm].clone()

            else:
                # Evaluate the model with the current weights
                ppl_dev, loss_dev = evaluate_model(dev_loader, criterion_eval, model)
                ppl_dev_list.append(ppl_dev)
                losses_dev.append(np.asarray(loss_dev).mean())
                pbar.set_description(f"lr= {params['lr']} ASGD= {'t0' in optimizer.param_groups[0]} PPL: {ppl_dev}")

                # Check if its time to switch to ASGD
                if len(ppl_dev_list) > threshold and ppl_dev > min(ppl_dev_list[:-threshold]):
                    params["lr"] *= 4/10
                    optimizer.param_groups[0]['lr'] = params["lr"]
                    print(epoch)
                    optimizer = torch.optim.ASGD(model.parameters(), lr=params["lr"], t0=0, lambd=0., weight_decay=1.2e-06)
                
            if  ppl_dev < best_ppl: # the lower, the better
                best_ppl = ppl_dev
                best_model = copy.deepcopy(model).to('cpu')
                patience = count_patience
            else:
                patience -= 1

            if patience <= 0: # Early stopping with patience
                break # Not nice but it keeps the code clean

        # Final evaluation
        best_model.to(device)
        final_ppl, _ = evaluate_model(test_loader, criterion_eval, best_model)
        print('Test PPL: ', final_ppl)
        store_model_with_params_and_lr(best_model, optimizer, epoch, exp_name, params, final_ppl)
        plot_losses(losses_train, losses_dev, sampled_epochs, exp_name, save=True)
        plot_ppl(ppl_dev_list, sampled_epochs, exp_name, save=True) 

